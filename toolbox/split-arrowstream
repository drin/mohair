#!/usr/bin/env python

# ------------------------------
# License

# Copyright 2024 Aldrin Montana
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# ------------------------------
# Overview
"""
Write a query plan in substrait using ibis.
"""


# ------------------------------
# Dependencies

# >> Standard modules
import os
from pathlib import Path

# >> Third-party
import pyarrow
import pandas
import ibis

from ibis import udf
from ibis_substrait.compiler.core import SubstraitCompiler
from ibis_substrait.compiler.core import stp as substrait # substrait.plan_pb2 module


# >> Internal
from mohair.util import ArgparseBuilder
from mohair.query.relations import ( SkyDomain
                                    ,SkyPartition
                                    ,SkyPartitionMeta
                                    ,SkyPartitionSlice)

# >> Functions
from mohair.util import ( TableFromBinary
                         ,SchemaFromBinary
                         ,BinaryFromSchema
                         ,BinaryFromRecordBatch)


# ------------------------------
# Module variables
SAMPLE_FPATH  = Path('resources') / 'sample-data.tsv'
SAMPLE_SCHEMA = pyarrow.schema([
     pyarrow.field('gene_id'   , pyarrow.string())
    ,pyarrow.field('cell_id'   , pyarrow.string())
    ,pyarrow.field('expression', pyarrow.float32())
])


# ------------------------------
# Parse CLI arguments first
if __name__ == '__main__':
    parsed_args, extra_args = (
        ArgparseBuilder.with_description('Write a substrait plan for a sample query')
                       .add_skytether_domain_arg(required=True)
                       .add_skytether_partition_arg(required=True)
                       .add_input_dir_arg(
                            required=True
                           ,help_str='Path to directory containing source domain'
                        )
                       .add_output_dir_arg(
                            required=True
                           ,help_str='Path to directory containing destination domain'
                        )
                       .parse_args()
    )


def TableFromTSV(data_fpath: Path = SAMPLE_FPATH) -> pyarrow.Table:
    """
    Convenience function that creates an arrow table from :data_fpath: using hard-coded
    assumptions.
    """

    # read data from the given file; assume 3 columns (see `SAMPLE_SCHEMA`)
    with open(data_fpath) as data_handle:
        col_names   = next(data_handle).split(' ')
        data_by_col = [ [] for _ in col_names ]

        for line in data_handle:
            fields = line.strip().split(' ')

            data_by_col[0].append(fields[0])
            data_by_col[1].append(fields[1])
            data_by_col[2].append(float(fields[2]))

    # construct the table and return it
    return pyarrow.Table.from_arrays(
         [
              pyarrow.array(data_by_col[0], type=pyarrow.string())
             ,pyarrow.array(data_by_col[1], type=pyarrow.string())
             ,pyarrow.array(data_by_col[2], type=pyarrow.float32())
         ]
        ,schema=SAMPLE_SCHEMA
    )

def WriteSubstraitToFile(substrait_msg: substrait.Plan) -> str:
    with substrait_fpath.open('wb') as file_handle:
        file_handle.write(substrait_msg.SerializeToString())

    return str(substrait_fpath)


def EncodeMetaKey(key_name: str) -> bytes:
    """ Convenience function to encode a metadata key as utf-8. """
    return key_name.encode('utf-8')

def EncodePartitionCount(pcount: int)  -> bytes:
    """ Convenience function to encode partition count in a `size_t` size. """
    return pcount.to_bytes(8)

def EncodeStripeSize(stripe_size: int) -> bytes:
    """ Convenience function to encode partition count in a `uint8_t` size. """
    return stripe_size.to_bytes(1)


def SetPartitionData(sky_partition: SkyPartition, data_table: pyarrow.Table) -> SkyPartition:
    """
    Convenience function to set the data of :sky_partition:. This will overwrite data
    currently held by :sky_partition: with hard-coded assumptions.
    """

    # clear data in the partition, just to be sure
    sky_partition.slices = []

    # cache some data for use in this function
    partition_key = Path(sky_partition.domain.key) / sky_partition.meta.key

    # Create 4 slices, assuming 20 rows in slices of max chunk size 5
    for ndx, pslice in enumerate(data_table.to_batches(max_chunksize=5)):

        # Each slice is treated as a table for simplicity
        sky_partition.slices.append(
            SkyPartitionSlice(
                 slice_index=ndx
                ,key=f'{partition_key};{ndx}'
                ,data=pyarrow.Table.from_batches([pslice])
            )
        )

    # our partition is 3 columns wide, has 4 slices
    sky_partition.meta.slice_width = 3
    sky_partition.meta.slice_count = ndx + 1

    # set schema first, then update `schema_meta` and schema metadata
    sky_partition.meta.schema = data_table.schema
    sky_partition.meta.WithMetadata({
         EncodeMetaKey('partition_count'): EncodePartitionCount(ndx + 1)
        ,EncodeMetaKey('stripe_size')    : EncodeStripeSize(1)
    })

    return sky_partition


def TStatAccumulate(data_table):
    squared_expr = data_table['expr_val'] * data_table['expr_val']
    return (
        data_table.group_by(data_table['feature_name'])
                  .aggregate(
                        cell_count=data_table.count()
                       ,expr_total=(data_table['expr_val'].sum())
                       ,expr_sumsq=squared_expr.sum()
                   )
    )

def TStatCombine(left_table, right_table):
    return (
        left_table.join(right_table, ['feature_name'])
          [
               left_table['feature_name'].name('feature_name')
              ,(left_table['cell_count'] + right_table['cell_count']).name('cell_count')
              ,(left_table['expr_total'] + right_table['expr_total']).name('expr_total')
              ,(left_table['expr_sumsq'] + right_table['expr_sumsq']).name('expr_sumsq')
          ]
    )

def TStatComplete(data_table):
    expr_avg = data_table['expr_total'] / data_table['cell_count']
    expr_var = (data_table['expr_sumsq'] / data_table['cell_count']) - (expr_avg * expr_avg)
    return (
        data_table[
             data_table['feature_name']
            ,expr_avg.name('expr_avg')
            ,expr_var.name('expr_var')
        ]
    )


# ------------------------------
# Main logic

if __name__ == '__main__':
    # Initialize domain and partitions
    sky_domain = SkyDomain(parsed_args.domain_key)
    sky_partitions = [
        sky_domain.PartitionFor(partition_key)
        for partition_key in parsed_args.partition_keys
    ]

    # ibis_catalog = [(sky_partition.name(), None) for sky_partition in sky_partitions]

    # Populate schemas and optionally data
    for sky_partition in sky_partitions:
        dst_dpath = Path(parsed_args.output_dir)
        dst_fpath = dst_dpath / sky_partition.name()
        src_fpath = Path(parsed_args.input_dir)  / sky_partition.name()

        # Make sure destination directory exists
        if not dst_dpath.is_dir(): dst_dpath.mkdir()

        # Make sure destination domain exists
        dst_domainpath = dst_dpath / sky_domain.key
        if not dst_domainpath.is_dir(): dst_domainpath.mkdir()

        # Read the data from source files
        with src_fpath.open('rb') as src_handle:
            partition_data = TableFromBinary(src_handle.read())

        # Populate data
        sky_partition.SetData(partition_data)

        # Write the data to dest files

        #   |> first the metadata file
        with dst_fpath.open('wb') as pmeta_handle:
            pmeta_handle.write(BinaryFromSchema(sky_partition.schema()))

        #   |> then the data files
        for data_slice in sky_partition.slices:
            pslice_fpath = Path(f'{dst_fpath}-{data_slice.slice_index}')
            with pslice_fpath.open('wb') as pslice_handle:
                serialized_data = BinaryFromRecordBatch(data_slice.AsBatch())
                pslice_handle.write(serialized_data)

            print(f'[{src_fpath}]\n\t-> [{dst_fpath}]')

    print(f'Split [{len(sky_partitions)}] input partitions')
